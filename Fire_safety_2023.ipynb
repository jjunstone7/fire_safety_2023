{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.io as sio\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = 'C:/python/train1/data/'\n",
    "start_index = 2   # 시작하는 파일 인덱스\n",
    "end_index = 59   # 종료하는 파일 인덱스 \n",
    "data_merge =pd.DataFrame()\n",
    "for index in range(start_index, end_index + 1):\n",
    "    file_index = index\n",
    "    file_path = f\"{base_path}2023-02-07_17-29-{file_index:02d}.615.mat\"\n",
    "    try:\n",
    "        raw_data = sio.loadmat(file_path)\n",
    "        signals=raw_data['signals']\n",
    "        data = signals.T\n",
    "        res = pd.DataFrame()\n",
    "        for j in range(100):\n",
    "            data_max = np.max(data[10000 * j : (10000 * j + 10000),:],axis =0)\n",
    "            data_max2 = data_max.reshape(1,-1)\n",
    "            df_max = pd.DataFrame(data_max2)\n",
    "            res = pd.concat([res,df_max], axis = 0)\n",
    "        data_merge=pd.concat([data_merge,res],axis=0)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "start_index = 0   # 시작하는 파일 인덱스\n",
    "end_index = 43  # 종료하는 파일 인덱스 \n",
    "for index in range(start_index, end_index + 1):\n",
    "    file_index = index\n",
    "    file_path = f\"{base_path}2023-02-07_17-30-{file_index:02d}.615.mat\"\n",
    "    try:\n",
    "        raw_data = sio.loadmat(file_path)\n",
    "        signals=raw_data['signals']\n",
    "        data = signals.T\n",
    "        res = pd.DataFrame()\n",
    "        for j in range(100):\n",
    "            data_max = np.max(data[10000 * j : (10000 * j + 10000),:],axis =0)\n",
    "            data_max2 = data_max.reshape(1,-1)\n",
    "            df_max = pd.DataFrame(data_max2)\n",
    "            res = pd.concat([res,df_max], axis = 0)\n",
    "        data_merge=pd.concat([data_merge,res],axis=0)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "print(data_merge.shape)\n",
    "\n",
    "base_path = 'C:/python/train3/train3/'\n",
    "start_index = 16   # 시작하는 파일 인덱스\n",
    "end_index = 59   # 종료하는 파일 인덱스 \n",
    "data_merge2 =pd.DataFrame()\n",
    "for index in range(start_index, end_index + 1):\n",
    "    file_index = index\n",
    "    file_path = f\"{base_path}2023-02-07_16-45-{file_index:02d}.148.mat\"\n",
    "    try:\n",
    "        raw_data = sio.loadmat(file_path)\n",
    "        signals=raw_data['signals']\n",
    "        data = signals.T\n",
    "        res = pd.DataFrame()\n",
    "        for j in range(100):\n",
    "            data_max = np.max(data[10000 * j : (10000 * j + 10000),:],axis =0)\n",
    "            data_max2 = data_max.reshape(1,-1)\n",
    "            df_max = pd.DataFrame(data_max2)\n",
    "            res = pd.concat([res,df_max], axis = 0)\n",
    "        data_merge2=pd.concat([data_merge2,res],axis=0)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "start_index = 0   # 시작하는 파일 인덱스\n",
    "end_index = 59  # 종료하는 파일 인덱스 \n",
    "for index in range(start_index, end_index + 1):\n",
    "    file_index = index\n",
    "    file_path = f\"{base_path}2023-02-07_16-46-{file_index:02d}.148.mat\"\n",
    "    try:\n",
    "        raw_data = sio.loadmat(file_path)\n",
    "        signals=raw_data['signals']\n",
    "        data = signals.T\n",
    "        res = pd.DataFrame()\n",
    "        for j in range(100):\n",
    "            data_max = np.max(data[10000 * j : (10000 * j + 10000),:],axis =0)\n",
    "            data_max2 = data_max.reshape(1,-1)\n",
    "            df_max = pd.DataFrame(data_max2)\n",
    "            res = pd.concat([res,df_max], axis = 0)\n",
    "        data_merge2=pd.concat([data_merge2,res],axis=0)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "        \n",
    "start_index = 0   # 시작하는 파일 인덱스\n",
    "end_index = 6  # 종료하는 파일 인덱스 \n",
    "for index in range(start_index, end_index + 1):\n",
    "    file_index = index\n",
    "    file_path = f\"{base_path}2023-02-07_16-47-{file_index:02d}.148.mat\"\n",
    "    try:\n",
    "        raw_data = sio.loadmat(file_path)\n",
    "        signals=raw_data['signals']\n",
    "        data = signals.T\n",
    "        res = pd.DataFrame()\n",
    "        for j in range(100):\n",
    "            data_max = np.max(data[10000 * j : (10000 * j + 10000),:],axis =0)\n",
    "            data_max2 = data_max.reshape(1,-1)\n",
    "            df_max = pd.DataFrame(data_max2)\n",
    "            res = pd.concat([res,df_max], axis = 0)\n",
    "        data_merge2=pd.concat([data_merge2,res],axis=0)\n",
    "    except FileNotFoundError:\n",
    "        pass        \n",
    "print(data_merge2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create_LSTM_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        w = X.iloc[(i + time_steps),:] \n",
    "        ys.append(w)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "\n",
    "X_train, y_train = create_dataset(data_merge, time_steps)\n",
    "y_train_expanded = np.repeat(y_train[:, np.newaxis, :],4, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM + Autoencoder\n",
    "inputs = Input(shape=(4,4))\n",
    "lstm1 = LSTM(128)(inputs)\n",
    "rep_layer = RepeatVector(4)(lstm1)\n",
    "lstm2 = LSTM(128,return_sequences=True)(rep_layer)\n",
    "time_layer = TimeDistributed(Dense(4,activation='linear'))(lstm2)\n",
    "model=Model(inputs = inputs,outputs = time_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#model.fit(X_train,y_train_expanded,epochs=10,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 4, 4)]            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               68096     \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  (None, 4, 128)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 4, 128)            131584    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 4, 4)             516       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200,196\n",
      "Trainable params: 200,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leak_detection(signals):\n",
    "    # signals : 4 x 1000000 행렬\n",
    "    \n",
    "    signals_T = signals.T\n",
    "    df_signal = pd.DataFrame(signals_T)\n",
    "    x_test = pd.DataFrame()\n",
    "    for i in range(100):\n",
    "        res = np.max(df_signal.iloc[i*10000:(i*10000+10000),:],axis=0)\n",
    "        res2 = pd.DataFrame(res).T\n",
    "        x_test = pd.concat([x_test,res2],axis=0)\n",
    "    \n",
    "    # 누출 감지 알고리즘 구현 (참가자 구현 부분)\n",
    "    res3 = create_dataset2(x_test,4)\n",
    "    y_pred = model.predict(res3)\n",
    "    y_pred_mean=np.mean(y_pred,axis=1)\n",
    "    x_test_mean = np.mean(res3,axis=1)\n",
    "    \n",
    "    test_loss = pd.DataFrame(np.mean(np.abs(y_pred_mean - x_test_mean), axis=1), columns=['Error'])\n",
    "    test_loss['threshold'] = 0.002 # 임계값\n",
    "    test_loss['anomaly'] = test_loss.Error > test_loss.threshold\n",
    "    sum_anomaly = np.sum(test_loss['anomaly'])\n",
    "    # 결과값 : 누출 (Y), 정상 (N)\n",
    "    if sum_anomaly > 10:\n",
    "        leakage = 'Y'\n",
    "    else:\n",
    "        leakage = 'N'\n",
    "    return leakage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
